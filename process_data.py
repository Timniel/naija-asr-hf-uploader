# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GBAPfAZxxigociBk7Twb3IsOPf6PMpIP
"""

# 1. Install necessary libraries
!pip install datasets huggingface_hub librosa soundfile pandas

# 2. Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

from huggingface_hub import notebook_login
notebook_login()

import os
import pandas as pd
from datasets import Dataset, Audio
from huggingface_hub import HfApi

# ==========================================
# 1. CONFIGURATION (UPDATE THESE PATHS!)
# ==========================================

# Path to folder containing 'pidgin_0001.wav', etc.
AUDIO_FOLDER_PATH = "/content/drive/MyDrive/clips"

# Path to your Excel or CSV file (Column A = Text)
TRANSCRIPT_FILE_PATH = "/content/drive/MyDrive/transcripts.xlsx"

# Your Hugging Face Repo ID
REPO_ID = "timniel/Pidgin_ASR_Dataset_Combined"

# ==========================================
# 2. PREPARE THE DATA
# ==========================================

print("Loading transcript file...")

# Load file. header=None means we treat the first row as data, not titles.
if TRANSCRIPT_FILE_PATH.endswith('.csv'):
    df = pd.read_csv(TRANSCRIPT_FILE_PATH, header=None)
else:
    df = pd.read_excel(TRANSCRIPT_FILE_PATH, header=None)

# Keep only the first column and name it 'text'
df = df.iloc[:, [0]]
df.columns = ['text']

print(f"Loaded {len(df)} transcripts.")

# --- LOGIC: Row 1 -> pidgin_0001.wav ---
def generate_audio_path(index):
    # index + 1 to start at 1. :04d means pad with zeros (0001)
    filename = f"pidgin_{index + 1:04d}.wav"
    return os.path.join(AUDIO_FOLDER_PATH, filename)

# Create the absolute path column
df['audio'] = df.index.map(generate_audio_path)

# --- VERIFY FILES EXIST ---
def file_exists(path):
    return os.path.exists(path)

print("Verifying audio files exist...")
initial_count = len(df)
df = df[df['audio'].apply(file_exists)]
final_count = len(df)

if final_count < initial_count:
    print(f"âš ï¸ Warning: {initial_count - final_count} audio files were not found and skipped.")
else:
    print("âœ… All audio files matched successfully!")

# ==========================================
# 3. CREATE DATASET
# ==========================================

print("Creating Hugging Face Dataset object...")
dataset = Dataset.from_pandas(df)

# Cast 'audio' column to Audio feature
dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))

# ==========================================
# 4. DEFINE UPDATED README (CORRECTED)
# ==========================================

yaml_metadata = """
---
language:
- pcm
license: cc-by-sa-4.0
task_categories:
- automatic-speech-recognition
tags:
- audio
- speech
- naija
- pidgin
dataset_info:
  features:
  - name: audio
    dtype: audio
  - name: text
    dtype: string
size_categories:
- 1K<n<10K
---
"""

readme_body = """# **Naija-ASR-Corpus v1.0 (NAC-v1.0)**

*A Foundational Automatic Speech Recognition Corpus for Nigerian Pidgin (Naija, PCM)*

---

## ðŸ“Œ **Dataset Summary**

**Naija-ASR-Corpus (NAC-v1.0)** is a speech dataset derived from the **Universal Dependencies Naija Spoken Corpus (UD_Naija-NSC)**.

The **NAC Team** processed the original long-form recordings by:
1.  **Segmenting** the audio into sentence-level clips.
2.  **Transcribing/Aligning** the text to create paired audio-text data suitable for ASR training.

This dataset is designed to support the development of Automatic Speech Recognition (ASR) systems for **Nigerian Pidgin (Naija, ISO 639-3: PCM)**.

---

## ðŸŽ¯ **Use Cases**

* Automatic speech recognition (ASR)
* Speech-to-text research
* Linguistic analysis of segmented Naija speech

---

## ðŸŒ **Language**

* **Language:** Nigerian Pidgin (Naija)
* **ISO 639-3 Code:** **PCM**

---

## ðŸ§© **Dataset Composition**

| Component              | Description                                           |
| ---------------------- | ----------------------------------------------------- |
| **Total Utterances**   | 5,883 processed speech clips                          |
| **Audio Format**       | `.wav` files, mono, 16kHz                             |
| **Transcripts**        | UTF-8 text                                            |

---

## ðŸ› ï¸ **Data Source & Attribution**

**Source Material:**
This dataset is entirely derived from the **Universal Dependencies Naija Spoken Corpus (UD_Naija-NSC)**.

**modifications by NAC Team:**
The original continuous recordings were split into shorter clips (`pidgin_0001.wav` to `pidgin_5883.wav`) and matched with specific transcripts to facilitate machine learning tasks.

**Required Attribution:**
> This dataset contains content derived from *UD_Naija-NSC (Universal Dependencies Naija Spoken Corpus)*, licensed under **CC-BY-SA 4.0**.
> *   **Original Authors:** UD_Naija-NSC contributors.
> *   **Original License:** [https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/)

---

## ðŸ“œ **Licensing**

Since this dataset is a derivative work of UD_Naija-NSC (CC-BY-SA 4.0), this dataset is released under the **Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)** license.

You are free to:
*   **Share** â€” copy and redistribute the material in any medium or format.
*   **Adapt** â€” remix, transform, and build upon the material for any purpose, even commercially.

**Under the following terms:**
*   **Attribution** â€” You must give appropriate credit to **UD_Naija-NSC** and the **NAC Team**.
*   **ShareAlike** â€” If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.

---

## ðŸ‘¥ **Contributors**

### **NAC Team (Dataset Curation & Processing)**
*   **Team Lead:** Augustine, Silver, Timmy
*   **Contributors:** Bryan, Ekene, Emmanuella, Shamsa

### **Original Data Creators**
*   **UD_Naija-NSC Team** (Universal Dependencies)
"""

full_readme = yaml_metadata.strip() + "\n" + readme_body

# ==========================================
# 5. PUSH TO HUB
# ==========================================

print(f"Pushing dataset to {REPO_ID}...")

# Push Data
dataset.push_to_hub(REPO_ID, private=False)

# Push README
api = HfApi()
api.upload_file(
    path_or_fileobj=full_readme.encode(),
    path_in_repo="README.md",
    repo_id=REPO_ID,
    repo_type="dataset"
)

print("âœ… Upload Complete! Check your dataset here:")
print(f"https://huggingface.co/datasets/{REPO_ID}")